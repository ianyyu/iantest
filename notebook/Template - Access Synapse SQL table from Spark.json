{
	"name": "Template - Access Synapse SQL table from Spark",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2
		},
		"metadata": {
			"language_info": {
				"name": "scala"
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Title\n",
					"The title of the notebook.\n",
					"\n",
					"## Purpose\n",
					"State the purpose of the notebook.\n",
					"\n",
					"## Methodology\n",
					"Quickly describe assumptions and processing steps.\n",
					"\n",
					"\n",
					"## Results\n",
					"Describe and comment the most important results.\n",
					"\n",
					"## Next steps\n",
					"State suggested next steps, based on results obtained in this notebook.\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Pre-requisites\n",
					"You need to be db_owner to read and write in sql pool. Ask your admin to run the following command with your AAD credential:\n",
					"\n",
					"    \n",
					"    EXEC sp_addrolemember 'db_owner', 'AAD@contoso.com'\n",
					"\n",
					"\n",
					"## Setup\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"// import libararies\n",
					"\n",
					"import org.apache.spark.sql.SqlAnalyticsConnector._\n",
					"import com.microsoft.spark.sqlanalytics.utils.Constants"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Parameters\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"val sql_pool_name = \"Your sql pool name\" //fill in your sql pool name\n",
					"val schema = \"Your schema name\" //fill in your schema name\n",
					"val table_read = \"Your sql table name to read data from\" //fill in your sql table read name\n",
					"val table_write = \"Your sql table name to write data to\" //fill in your sql table write name\n",
					"\n",
					"val account_name = \"Your storage account name\" //fill in your storage account name\n",
					"val temp_folder = \"Your temp folder\" //fill in a container name under your storage account"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Read from a SQL Pool table with Spark"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"// Read the table in the sql pool as a Spark dataframe\n",
					"val spark_read = spark.read.\n",
					"    option(Constants.TEMP_FOLDER, s\"abfss://$temp_folder@$account_name.dfs.core.windows.net/\").\n",
					"    sqlanalytics(s\"$sql_pool_name.$schema.$table_read\")\n",
					"\n",
					"spark_read.show(5, truncate = false)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Write a Spark dataframe into your sql pool\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"// Write the Spark dataframe to the table in the sql pool\n",
					"df.write.option(Constants.TEMP_FOLDER, s\"abfss://$temp_folder@$account_name.dfs.core.windows.net/\")\n",
					"    .sqlanalytics(s\"$sql_pool_name.$schema.$table_write\", Constants.INTERNAL)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"Now open Synapse object explorer and go to **Data**->**Databases**->**<your sql pool name>**->**Tables**, you will see the new table show up there."
				]
			}
		]
	}
}